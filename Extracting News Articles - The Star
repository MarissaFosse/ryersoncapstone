#Toronto Star

import requests
from bs4 import BeautifulSoup

#list of 147 Toronto Star articles to extract content 
#Or, use Url searching Toronto Star for articles containing "Toronto" + "Police"?

URL = 'https://www.thestar.com/news/gta/2019/12/31/with-291-people-shot-2019-is-closing-as-torontos-bloodiest-year-on-record-for-overall-gun-violence.html'

page = requests.get(URL)

soup = BeautifulSoup(page.content, 'html.parser')

results = soup.find(id='body-content')

article = results.find_all('div', class_='c-article-body__content')

#Trial: find all paragraphs

for paragraph in article:
    parag_elem = paragraph.find('p', class_='text-block-container')
    print(parag_elem.text.strip())
    print()
	
#above returns only first line - how do I get it to iterate through each <p class="text-block-container">?

#Step 1: return article heading
h1_tag = soup.find('h1')
title = h1_tag.text
print(title)

#Step 2: return publishing date
pub_date = soup.find('span', class_='article__published-date')
pub_date = pub_date.text
print(pub_date)


#Step 3: find all paragraphs

p_tags = soup.find_all('p', class_='text-block-container')
p_tags_text = [tag.get_text().strip() for tag in p_tags]
sentence_list = [sentence for sentence in p_tags_text if not '\n' in sentence]
sentence_list = [sentence for sentence in sentence_list if '.' in sentence]
article = ' '.join(sentence_list)

#Step 4: extract all unordered list items (may not include)

li_tags = soup.find_all('li')
li_tags_text = [tag.get_text().strip() for tag in li_tags]
bullet_list = [sentence for sentence in li_tags_text if not '\n' in sentence]
bullet_list = [sentence for sentence in bullet_list if '.' in sentence]
points = ' '.join(bullet_list)

# How do I merge article and points to one text set?
# Need to determine how I can pass list of urls through function that will extract <title>, <pub_date>, and <article> text into a dataframe... 

#Step 5: create a data frame

import pandas as pd 

scraped_data = pd.DataFrame({'title': title, 'date': pub_date, 'content': article})
scraped_data.head()


#tokenize, stemming, vectorization

#Once data frame exists, then I need to determine how to categorize the articles into one of 10 TPS policy portfolios: (Violent Crime, Community Policing & Demographics, GIS & Technology, Missing Persons, Open Data & Visualization, Overdoses & Cannabis, Persons in Crisis, Race-Based Data Collection, Staffing & Workload, Traffic) + perhaps a catch-all category: Law Enforcement



