# -*- coding: utf-8 -*-
"""TPSMedia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RS-4P3uJ9zeRLbdKsBNT4D1gaFTpBjRF
"""

import requests
import nltk
from bs4 import BeautifulSoup
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
import random

URL = 'https://www.thestar.com/news/gta/2019/12/31/with-291-people-shot-2019-is-closing-as-torontos-bloodiest-year-on-record-for-overall-gun-violence.html'

page = requests.get(URL)
soup = BeautifulSoup(page.content, 'html.parser')
soup.title.text

"""Code below works for Toronto Star articles. Try to modify for Toronto Sun too."""

results = soup.find(class_='c-article-body__content') 
#print(results.prettify())
results_text = [tag.get_text().strip() for tag in results]
sentence_list = [sentence for sentence in results_text if not '\n' in sentence]
sentence_list = [sentence for sentence in sentence_list if '.' in sentence]
article = ' '.join(sentence_list)
print(article)

tokens = [t for t in article.split()]
print(tokens)

from nltk.tokenize import sent_tokenize, word_tokenize
print(word_tokenize(article))

word_tokens = word_tokenize(article)
stop_words = set(stopwords.words('english'))
filtered_article = [w for w in word_tokens if not w in stop_words]
filtered_sentence = []

for w in word_tokens:
  if w not in stop_words:
    filtered_sentence.append(w)

print(filtered_sentence)

clean_tokens = tokens[:]

for token in tokens:
  if token in stopwords.words('english'):
    
    clean_tokens.remove(token)

freq = nltk.FreqDist(clean_tokens)
for key,val in freq.items():
  print(str(key) + ':' + str(val))

freq.plot(20, cumulative=False)

"""Need to determine how to lemmatize my data set. The below code doesn't work with a list."""

lemmatizer = WordNetLemmatizer()

print(lemmatizer.lemmatize(filtered_sentence))

"""Building the dataframe: tstar_articles"""

import pandas as pd
url_file = 'https://github.com/MarissaFosse/ryersoncapstone/raw/master/DailyNewsArticles.xlsx'

tstar_articles = pd.read_excel(url_file, "TorontoStar Articles", header=0) 
#print (tstar_articles)

tstar_articles.describe()

tstar_articles.head()

"""**Using the dataframe above, how can I write a Python script to open the URL, scrape the text, append the article text to the row as another column (or perhaps a linked text file?) and then move to the next row until all 147 have been scraped?**"""

nltk.download('movie_reviews')
from nltk.corpus import movie_reviews

documents = [(list(movie_reviews.words(fileid)), category)
             for category in movie_reviews.categories()
             for fileid in movie_reviews.fileids(category)]

#documents = []
#for category in movie_reviews.categories():
#   for fileid in movie_reviews.fileids(category):
#     documents.append(movie_reviews.words(fileid), category)


random.shuffle(documents)

#print(documents[1])

all_words = []
for w in movie_reviews.words():
    all_words.append(w.lower())

all_words = nltk.FreqDist(all_words)

word_features = list(all_words.keys())[:3000]

def find_features(document):
  words = set(document) #a set is one iteration of all words
  features = {}
  for w in word_features:
    features[w] = (w in words)
  return features

print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))

featuresets = [(find_features(rev), category) for (rev, category) in documents]

training_set = featuresets[:1900] #up to but not including 1900'th review
testing_set = featuresets[1900:]

classifier = nltk.NaiveBayesClassifier.train(training_set)
print("Classifier accuracy percent:",(nltk.classify.accuracy(classifier, testing_set))*100)